{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f1dReIWK1AD",
        "outputId": "07f846ed-582c-4069-b7cd-6e84c0c386f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Loading the data\n",
        "file_path = '/content/drive/MyDrive/a5_data/'\n",
        "\n",
        "# Define file paths\n",
        "train_dir = '/content/drive/MyDrive/a5_data/train'\n",
        "val_dir = '/content/drive/MyDrive/a5_data/val'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br5p2YryS-no"
      },
      "source": [
        "#Initial Evaluations of Custom Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS6Q3A-uSkde"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Baseline transformations for data consistency\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Random transformations for training data that may offer improved model performance\n",
        "rand_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomResizedCrop(128),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Prepare data sets without random transformations\n",
        "train_set = torchvision.datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "val_set = torchvision.datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Prepare data sets with random transformations applied\n",
        "train_set_rand = torchvision.datasets.ImageFolder(root=train_dir, transform=rand_transform)\n",
        "train_loader_rand = torch.utils.data.DataLoader(train_set_rand, batch_size=64, shuffle=True)\n",
        "val_set_rand = torchvision.datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "val_loader_rand = torch.utils.data.DataLoader(val_set_rand, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# A baseline CNN model\n",
        "class BaselineCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# A model with normalization\n",
        "class LayerNormCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LayerNormCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln1 = nn.LayerNorm([16, 64, 64]) \n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln2 = nn.LayerNorm([32, 32, 32]) \n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln3 = nn.LayerNorm([64, 16, 16]) \n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.ln1(x))\n",
        "\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.ln2(x))\n",
        "\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.ln3(x))\n",
        "\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# A residual connections model\n",
        "class ResConn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResConn, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=3)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = self.conv1(x) \n",
        "        x = self.pool(torch.relu(residual))\n",
        "\n",
        "        residual = self.conv2(x)\n",
        "        x = self.pool(torch.relu(self.conv2(x)) + residual)\n",
        "\n",
        "        residual = self.conv3(x)\n",
        "        x = self.pool(torch.relu(self.conv3(x)) + residual)\n",
        "\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def plot_loss_accuracy(train_losses, val_losses, train_assc, val_accs, model_name):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Plot loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, label='Validation  Loss')\n",
        "    plt.title(f'{model_name} Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy curves\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_assc, label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accs, label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, classes, model_name):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.numpy())\n",
        "            y_pred.extend(predicted.numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def compare_models(names, accuracies):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(names, accuracies)\n",
        "    plt.xticks(rotation=10, ha='right')  # Tilt the x-axis labels\n",
        "    plt.title('Model Comparison')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "def train_model(model, criterion, optimizer, train, val, model_name):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    best_acc = 0\n",
        "    best_epoch = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for inputs, labels in train:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_accuracy)\n",
        "\n",
        "        val_loss, val_accuracy = evaluate(model, val)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "        if val_accuracy > best_acc:\n",
        "            best_acc = val_accuracy\n",
        "            best_epoch = epoch + 1\n",
        "            # Save the model's state dictionary\n",
        "            torch.save(model.state_dict(), f'{model_name}.pth')\n",
        "    print(f\"Best accuracy for {model_name}, epoch: {best_epoch}:\", {best_acc})\n",
        "    plot_loss_accuracy(train_losses, val_losses, train_accs, val_accs, model_name)\n",
        "\n",
        "    # Save train_losses, val_losses, train_accs, val_accs into a file\n",
        "    data = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs\n",
        "    }\n",
        "    torch.save(data, f'{model_name}_metrics.pth')\n",
        "\n",
        "    return best_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()     # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "baseline_model = BaselineCNN()\n",
        "baseline_model_rand = BaselineCNN()\n",
        "\n",
        "layernorm_model = LayerNormCNN()\n",
        "layernorm_model_rand = LayerNormCNN()\n",
        "\n",
        "resconn_model = ResConn()\n",
        "resconn_model_rand = ResConn()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer initialization\n",
        "optimizer_baseline = optim.SGD(baseline_model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_baseline_rand = optim.SGD(baseline_model_rand.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "optimizer_layernorm = optim.SGD(layernorm_model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_layernorm_rand = optim.SGD(layernorm_model_rand.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "optimizer_resconn = optim.SGD(resconn_model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_resconn_rand = optim.SGD(resconn_model_rand.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train each model variant\n",
        "best_acc_bs = train_model(baseline_model, criterion, optimizer_baseline, train_loader, val_loader, \"Baseline Model\")\n",
        "best_acc_bsrt = train_model(baseline_model_rand, criterion, optimizer_baseline_rand, train_loader_rand, val_loader_rand,\n",
        "                            \"Baseline Model with random transformation\")\n",
        "\n",
        "best_acc_lm = train_model(layernorm_model, criterion, optimizer_layernorm, train_loader, val_loader, \"Layer Normalization Model\")\n",
        "best_acc_lmrt = train_model(layernorm_model_rand, criterion, optimizer_layernorm_rand, train_loader_rand, val_loader_rand,\n",
        "                            \"Layer Normalization Model with random transformation\")\n",
        "\n",
        "best_acc_rc = train_model(resconn_model, criterion, optimizer_resconn, train_loader, val_loader, \"Residual Connection Model\")\n",
        "best_acc_rcrt = train_model(resconn_model_rand, criterion, optimizer_resconn_rand, train_loader_rand, val_loader_rand,\n",
        "                            \"Residual Connection Model with random transformation\")\n",
        "\n",
        "# Define classes for confusion matrix\n",
        "classes = train_set.classes\n",
        "\n",
        "# Load trained models\n",
        "baseline_model.load_state_dict(torch.load('Baseline Model.pth'))\n",
        "baseline_model_rand.load_state_dict(torch.load('Baseline Model with random transformation.pth'))\n",
        "layernorm_model.load_state_dict(torch.load('Layer Normalization Model.pth'))\n",
        "layernorm_model_rand.load_state_dict(torch.load('Layer Normalization Model with random transformation.pth'))\n",
        "resconn_model.load_state_dict(torch.load('Residual Connection Model.pth'))\n",
        "resconn_model_rand.load_state_dict(torch.load('Residual Connection Model with random transformation.pth'))\n",
        "\n",
        "# Generate confusion matrix for each model variant\n",
        "plot_confusion_matrix(baseline_model, val_loader, classes, 'Baseline Model')\n",
        "plot_confusion_matrix(baseline_model_rand, val_loader_rand, classes, 'Baseline Model with random transformation')\n",
        "plot_confusion_matrix(layernorm_model, val_loader, classes, 'Layer Normalization Model')\n",
        "plot_confusion_matrix(layernorm_model_rand, val_loader_rand, classes, 'Layer Normalization Model with random transformation')\n",
        "plot_confusion_matrix(resconn_model, val_loader, classes, 'Residual Connection Model')\n",
        "plot_confusion_matrix(resconn_model_rand, val_loader_rand, classes, 'Residual Connection Model with random transformation')\n",
        "\n",
        "# Compare models based on accuracy\n",
        "names = ['Baseline Model', 'Baseline Model w/ random transformation', 'Layer Normalization Model',\n",
        "         'Layer Normalization Model w/ random transformation', 'Residual Connection Model',\n",
        "         'Residual Connection Model w/ random transformation']\n",
        "# dataloaders = [val_loader, val_loader_rand, val_loader, val_loader_rand, val_loader, val_loader_rand]\n",
        "best_accuracies = [best_acc_bs, best_acc_bsrt, best_acc_lm, best_acc_lmrt, best_acc_rc, best_acc_rcrt]\n",
        "compare_models(names, best_accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCs6pC9dTCok"
      },
      "source": [
        "#ResNet18 - Pre-trained Model Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoTvz_MgTHRm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import ssl\n",
        "\n",
        "\n",
        "file_path = './a5_data/'\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Defining random transformations for training data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomResizedCrop(128),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Baseline transformations for validation data consistency \n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(file_path + 'train', transform=transform_train)\n",
        "val_data = datasets.ImageFolder(file_path + 'val', transform=transform_val)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=8, shuffle=False)\n",
        "\n",
        "# Define the Model Architecture\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)  # For the 2 classes: MEL and NV\n",
        "\n",
        "# Defining Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Learning Rate\n",
        "\n",
        "# Training Loop\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 200  # Number of training epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # 5. Validation of current epoch model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy on validation set: {100 * correct / total}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY-BD-zfTYxz"
      },
      "source": [
        "#Experimentation session, including saving models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agIub4uBTcvk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "\n",
        "# Loading the data\n",
        "file_path = '/content/drive/MyDrive/a5_data/'\n",
        "model_path = '/content/drive/MyDrive/a5_models/'\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        if self.downsample is None and in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SimpleCNNBinary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNNBinary, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.resblock1 = ResidualBlock(32, 64, stride=2)\n",
        "        self.resblock2 = ResidualBlock(64, 64)\n",
        "\n",
        "        dummy_input = torch.zeros(1, 3, 128, 128)\n",
        "        dummy_output = self.forward_features(dummy_input)\n",
        "        linear_input_size = dummy_output.numel() / dummy_output.size(0)\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(int(linear_input_size), 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.resblock1(x)\n",
        "        x = self.resblock2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualLayerNormCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResidualLayerNormCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln1 = nn.LayerNorm([16, 64, 64]) \n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln2 = nn.LayerNorm([32, 32, 32])\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln3 = nn.LayerNorm([64, 16, 16])\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolutional block\n",
        "        out = torch.relu(self.conv1(x))\n",
        "        out = self.pool(out)\n",
        "        out = torch.relu(self.ln1(out))\n",
        "\n",
        "        # Second convolutional block\n",
        "        out = torch.relu(self.conv2(out))\n",
        "        out = self.pool(out)\n",
        "        out = torch.relu(self.ln2(out))\n",
        "\n",
        "        # Third convolutional block\n",
        "        out = torch.relu(self.conv3(out))\n",
        "        out = self.pool(out)\n",
        "        out = torch.relu(self.ln3(out))\n",
        "\n",
        "        # Fully connected layers\n",
        "        out = out.view(-1, 64 * 16 * 16)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Random transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomResizedCrop(128),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Data consistency transformations\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(file_path + 'train', transform=transform_train)\n",
        "val_data = datasets.ImageFolder(file_path + 'val', transform=transform_val)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=8, shuffle=False)\n",
        "\n",
        "# Preview the training images\n",
        "Xexamples, Yexamples = next(iter(train_loader))\n",
        "\n",
        "for i in range(8):\n",
        "    plt.subplot(2,4,i+1)\n",
        "    img = Xexamples[i].numpy().transpose(1, 2, 0)\n",
        "    plt.imshow(img, interpolation='none')\n",
        "    plt.title('NV' if Yexamples[i] else 'MEL')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResidualLayerNormCNN().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 200 # Number of training epochs\n",
        "\n",
        "# Open a file to save the epoch, loss, and accuracy data\n",
        "with open('./training_metrics.txt', 'w') as f:\n",
        "    f.write('Epoch,Loss,Validation Accuracy\\n')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():  # No gradient computation in evaluation phase\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
        "                outputs = model(inputs)\n",
        "                predicted = torch.sigmoid(outputs) > 0.5  # Convert to binary predictions\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).type(torch.float).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        # Save metrics to file\n",
        "        f.write(f'{epoch+1},{epoch_loss:.4f},{accuracy:.2f}\\n')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        # Save model checkpoint\n",
        "        if (epoch > 119): # Adjustment, models from epochs 0-119 rarely offered competetive results\n",
        "            torch.save(model, f'{model_path}model_wa_epoch_{epoch+1}.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
